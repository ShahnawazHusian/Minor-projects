{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92bcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def young_to_old(image_path, output_path):\n",
    "  # Read the image\n",
    "  img = cv2.imread(image_path)\n",
    "\n",
    "  # Convert to grayscale (optional, for some effects)\n",
    "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Apply effects\n",
    "  # (a) Reduce contrast and add slight blur to simulate wrinkles\n",
    "  blurred = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "  reduced_contrast = cv2.addWeighted(blurred, 0.8, img, 0.2, 0)\n",
    "\n",
    "  # (b) Add noise for a vintage photo effect\n",
    "  noise = np.random.randint(0, 25, img.shape, dtype=\"uint8\")\n",
    "  vintage_noisy = cv2.add(reduced_contrast, noise)\n",
    "\n",
    "#   (c) Add a sepia tone (optional)\n",
    "  sepia_kernel = np.array([[0.272, 0.534, 0.131],\n",
    "                          [0.349, 0.686, 0.168],\n",
    "                          [0.393, 0.769, 0.189]])\n",
    "  sepia_toned = cv2.filter2D(vintage_noisy, -1, sepia_kernel)\n",
    "\n",
    "#   Choose the desired effect or combine them for a more pronounced look\n",
    "  aged_image = vintage_noisy  # Or sepia_toned\n",
    "\n",
    "  # Save the result\n",
    "  cv2.imwrite(output_path, aged_image)\n",
    "\n",
    "# Example usage\n",
    "image_path = \"C:/image/download (srk).jpg\"\n",
    "output_path = \"C:/image/download (srkold).jpg\"\n",
    "young_to_old(image_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7db08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahn\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'frozenset' object has no attribute 'circle_radius'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n\u001b[0;32m     27\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m face_landmarks \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n\u001b[1;32m---> 28\u001b[0m     mp_drawing\u001b[38;5;241m.\u001b[39mdraw_landmarks(\n\u001b[0;32m     29\u001b[0m         image\u001b[38;5;241m=\u001b[39mimage, landmark_list\u001b[38;5;241m=\u001b[39mface_landmarks, connection_drawing_spec\u001b[38;5;241m=\u001b[39mmp_face_mesh\u001b[38;5;241m.\u001b[39mFACEMESH_CONTOURS,\n\u001b[0;32m     30\u001b[0m         landmark_drawing_spec\u001b[38;5;241m=\u001b[39mmp_face_mesh\u001b[38;5;241m.\u001b[39mFACEMESH_TESSELATION)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mediapipe\\python\\solutions\\drawing_utils.py:193\u001b[0m, in \u001b[0;36mdraw_landmarks\u001b[1;34m(image, landmark_list, connections, landmark_drawing_spec, connection_drawing_spec, is_drawing_landmarks)\u001b[0m\n\u001b[0;32m    190\u001b[0m drawing_spec \u001b[38;5;241m=\u001b[39m landmark_drawing_spec[idx] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    191\u001b[0m     landmark_drawing_spec, Mapping) \u001b[38;5;28;01melse\u001b[39;00m landmark_drawing_spec\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# White circle border\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m circle_border_radius \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(drawing_spec\u001b[38;5;241m.\u001b[39mcircle_radius \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    194\u001b[0m                            \u001b[38;5;28mint\u001b[39m(drawing_spec\u001b[38;5;241m.\u001b[39mcircle_radius \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1.2\u001b[39m))\n\u001b[0;32m    195\u001b[0m cv2\u001b[38;5;241m.\u001b[39mcircle(image, landmark_px, circle_border_radius, WHITE_COLOR,\n\u001b[0;32m    196\u001b[0m            drawing_spec\u001b[38;5;241m.\u001b[39mthickness)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Fill color into the circle\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'frozenset' object has no attribute 'circle_radius'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "image = cv2.imread(\"C:/image/download (srk).jpg\")\n",
    "\n",
    "# Load pre-trained face detector and facial landmark predictor\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,  # Set to 1 for single face detection\n",
    "    refine_landmarks=True,  # Refine landmarks for higher accuracy\n",
    "    min_detection_confidence=0.5,  # Minimum confidence score for detection\n",
    "    min_tracking_confidence=0.5  # Minimum confidence score for tracking\n",
    ")\n",
    "\n",
    "# Assuming you have an image loaded as 'image' (or video frame)\n",
    "\n",
    "# Convert the BGR image to RGB format (MediaPipe expects RGB)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Detect faces and landmarks\n",
    "results = face_mesh.process(image_rgb)\n",
    "\n",
    "# Draw the detected landmarks on the image (optional)\n",
    "if results.multi_face_landmarks:\n",
    "  for face_landmarks in results.multi_face_landmarks:\n",
    "    mp_drawing.draw_landmarks(\n",
    "        image=image, landmark_list=face_landmarks, connection_drawing_spec=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        landmark_drawing_spec=mp_face_mesh.FACEMESH_TESSELATION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7720168",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - imshow() missing required argument 'winname' (pos 1)\n>  - imshow() missing required argument 'winname' (pos 1)\n>  - imshow() missing required argument 'winname' (pos 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the input image\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimshow()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize MediaPipe face mesh\u001b[39;00m\n\u001b[0;32m      8\u001b[0m mp_face_mesh \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mface_mesh\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'imshow'\n> Overload resolution failed:\n>  - imshow() missing required argument 'winname' (pos 1)\n>  - imshow() missing required argument 'winname' (pos 1)\n>  - imshow() missing required argument 'winname' (pos 1)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load the input image\n",
    "image = cv2.imshow()\n",
    "\n",
    "# Initialize MediaPipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Convert the image to RGB\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Run MediaPipe face mesh\n",
    "with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, min_detection_confidence=0.5) as face_mesh:\n",
    "    # Process the image\n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "    # Draw face landmarks and contours\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Draw face landmarks\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=None,  # No predefined connections\n",
    "                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "                connection_drawing_spec=None  # No connection drawing spec\n",
    "            )\n",
    "\n",
    "if cv2.waitKey == 1 or 0xFF == ord(q):\n",
    "    break\n",
    "# Display the result\n",
    "cv2.imshow(\"Aged Image\", image)\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126d5ff",
   "metadata": {},
   "source": [
    "# grid like face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4c0f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahn\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)  # Change 0 to video file path if needed\n",
    "\n",
    "while True:\n",
    "  # Capture frame-by-frame\n",
    "  success, image = cap.read()\n",
    "\n",
    "  if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading video, check for end of stream\n",
    "      break\n",
    "\n",
    "  # Convert the BGR image to RGB format\n",
    "  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  # Run MediaPipe face mesh\n",
    "  with mp_face_mesh.FaceMesh(max_num_faces=1, min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "  # Draw face landmarks and contours (if faces detected)\n",
    "  if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=image,\n",
    "          landmark_list=face_landmarks,\n",
    "          connections=mp_face_mesh.FACEMESH_CONTOURS,  # Draw mesh connections\n",
    "          landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=1, circle_radius=1),\n",
    "          connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=1)\n",
    "      )\n",
    "\n",
    "  # Display the resulting frame\n",
    "  cv2.imshow('Facial Landmarks', image)\n",
    "\n",
    "  # Quit if 'q' key is pressed\n",
    "  if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "# Release capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88970f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face_bounding_box(face_landmarks):\n",
    "  \"\"\"\n",
    "  This function calculates the bounding box coordinates of a face based on facial landmarks.\n",
    "\n",
    "  Args:\n",
    "      face_landmarks: A list of facial landmark coordinates (e.g., from MediaPipe results).\n",
    "\n",
    "  Returns:\n",
    "      A tuple containing the top-left (x1, y1) and bottom-right (x2, y2) coordinates of the bounding box.\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize minimum and maximum coordinates\n",
    "  x_min, y_min = float('inf'), float('inf')\n",
    "  x_max, y_max = float('-inf'), float('-inf')\n",
    "\n",
    "  # Loop through facial landmarks and update min/max coordinates\n",
    "  for landmark in face_landmarks:\n",
    "    x, y = landmark.x, landmark.y\n",
    "    x_min = min(x_min, x)\n",
    "    y_min = min(y_min, y)\n",
    "    x_max = max(x_max, x)\n",
    "    y_max = max(y_max, y)\n",
    "\n",
    "  # Return the bounding box coordinates as a tuple\n",
    "  return (int(x_min), int(y_min)), (int(x_max), int(y_max))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cbb403",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NormalizedLandmarkList' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m face_landmarks \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Loop through each facial landmark in the list\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m landmark \u001b[38;5;129;01min\u001b[39;00m face_landmarks:\n\u001b[0;32m      7\u001b[0m       \u001b[38;5;66;03m# Access individual landmark information (e.g., x, y coordinates)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m       landmark_x \u001b[38;5;241m=\u001b[39m landmark\u001b[38;5;241m.\u001b[39mx\n\u001b[0;32m      9\u001b[0m       landmark_y \u001b[38;5;241m=\u001b[39m landmark\u001b[38;5;241m.\u001b[39my\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NormalizedLandmarkList' object is not iterable"
     ]
    }
   ],
   "source": [
    "# ... (Existing code for MediaPipe face mesh)\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "  for face_landmarks in results.multi_face_landmarks:\n",
    "    # Loop through each facial landmark in the list\n",
    "    for landmark in face_landmarks:\n",
    "      # Access individual landmark information (e.g., x, y coordinates)\n",
    "      landmark_x = landmark.x\n",
    "      landmark_y = landmark.y\n",
    "\n",
    "      # ... (Process or visualize each landmark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5e26a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahn\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)  # Change 0 to video file path if needed\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    success, image = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        # If loading video, check for end of stream\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB format\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Run MediaPipe face mesh\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1, min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        results = face_mesh.process(image_rgb)\n",
    "\n",
    "        # Draw face landmarks and contours (if faces detected)\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # Draw landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(0, 255, 0), thickness=1, circle_radius=1\n",
    "                    ),\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(255, 0, 0), thickness=1, circle_radius=1\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Facial Landmarks\", image)\n",
    "\n",
    "    # Quit if 'q' key is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410b31d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe face mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load dog ears and nose images\n",
    "dog_ears = cv2.imread(\"dog_ears.png\", cv2.IMREAD_UNCHANGED)\n",
    "dog_nose = cv2.imread(\"dog_nose.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)  # Change 0 to video file path if needed\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    success, image = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        # If loading video, check for end of stream\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB format\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Run MediaPipe face mesh\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1, min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        results = face_mesh.process(image_rgb)\n",
    "\n",
    "        # Draw face landmarks (for visualization)\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(0, 255, 0), thickness=1, circle_radius=1\n",
    "                    ),\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(255, 0, 0), thickness=1, circle_radius=1\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "                # Get landmark coordinates\n",
    "                landmarks = face_landmarks.landmark\n",
    "\n",
    "                # Overlay dog ears\n",
    "                ear_x = int(landmarks[2].x * image.shape[1])  # Example landmark index for left eye\n",
    "                ear_y = int(landmarks[2].y * image.shape[0])\n",
    "                image = overlay_transparent(image, dog_ears, ear_x, ear_y)\n",
    "\n",
    "                # Overlay dog nose\n",
    "                nose_x = int(landmarks[5].x * image.shape[1])  # Example landmark index for nose\n",
    "                nose_y = int(landmarks[5].y * image.shape[0])\n",
    "                image = overlay_transparent(image, dog_nose, nose_x, nose_y)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Snapchat Dog Filter\", image)\n",
    "\n",
    "    # Quit if 'q' key is pressed\n",
    "    if cv2.waitKey(5) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "def overlay_transparent(background_img, overlay_img, x, y):\n",
    "    # Extract the alpha mask of the RGBA image, convert to 3 channel\n",
    "    overlay_img = overlay_img[:, :, :3]\n",
    "    overlay_mask = overlay_img[:, :, 3:]\n",
    "\n",
    "    # Background and overlay images\n",
    "    background_img[y:y+overlay_img.shape[0], x:x+overlay_img.shape[1]] = (\n",
    "        overlay_mask * overlay_img[:, :, :3] + (1.0 - overlay_mask) * background_img[y:y+overlay_img.shape[0], x:x+overlay_img.shape[1]]\n",
    "    )\n",
    "    return background_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05d25e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahn\\anaconda3\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'mediapipe.python.solutions.face_mesh' has no attribute 'Landmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m face_landmarks \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mmulti_face_landmarks:\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;66;03m# Access specific landmark coordinates (modify for precise placement)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m         left_ear_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(face_landmarks\u001b[38;5;241m.\u001b[39mlandmark[mp_face_mesh\u001b[38;5;241m.\u001b[39mLandmark\u001b[38;5;241m.\u001b[39mLEFT_EAR_TIP]\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     38\u001b[0m         left_ear_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(face_landmarks\u001b[38;5;241m.\u001b[39mlandmark[mp_face_mesh\u001b[38;5;241m.\u001b[39mLandmark\u001b[38;5;241m.\u001b[39mLEFT_EAR_TIP]\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     39\u001b[0m         right_ear_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(face_landmarks\u001b[38;5;241m.\u001b[39mlandmark[mp_face_mesh\u001b[38;5;241m.\u001b[39mLandmark\u001b[38;5;241m.\u001b[39mRIGHT_EAR_TIP]\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'mediapipe.python.solutions.face_mesh' has no attribute 'Landmark'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe face mesh and drawing utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load dog ears and nose images (modify paths as needed)\n",
    "dog_ears_image = cv2.imread(\"dog_ears.png\", cv2.IMREAD_UNCHANGED)\n",
    "dog_nose_image = cv2.imread(\"dog_nose.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)  # Change 0 to video file path if needed\n",
    "\n",
    "while True:\n",
    "  # Capture frame-by-frame\n",
    "  success, image = cap.read()\n",
    "\n",
    "  if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      # If loading video, check for end of stream\n",
    "      break\n",
    "\n",
    "  # Convert the BGR image to RGB format\n",
    "  image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  # Run MediaPipe face mesh\n",
    "  with mp_face_mesh.FaceMesh(\n",
    "      max_num_faces=1, min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    "  ) as face_mesh:\n",
    "      results = face_mesh.process(image_rgb)\n",
    "\n",
    "      # Draw face landmarks and contours (if faces detected)\n",
    "      if results.multi_face_landmarks:\n",
    "          for face_landmarks in results.multi_face_landmarks:\n",
    "              # Access specific landmark coordinates (modify for precise placement)\n",
    "              left_ear_x = int(face_landmarks.landmark[mp_face_mesh.Landmark.LEFT_EAR_TIP].x * image.shape[1])\n",
    "              left_ear_y = int(face_landmarks.landmark[mp_face_mesh.Landmark.LEFT_EAR_TIP].y * image.shape[0])\n",
    "              right_ear_x = int(face_landmarks.landmark[mp_face_mesh.Landmark.RIGHT_EAR_TIP].x * image.shape[1])\n",
    "              right_ear_y = int(face_landmarks.landmark[mp_face_mesh.Landmark.RIGHT_EAR_TIP].y * image.shape[0])\n",
    "              nose_tip_x = int(face_landmarks.landmark[mp_face_mesh.Landmark.NOSE_TIP].x * image.shape[1])\n",
    "              nose_tip_y = int(face_landmarks.landmark[mp_face_mesh.Landmark.NOSE_TIP].y * image.shape[0])\n",
    "\n",
    "              # Resize dog ears and nose images based on face size (optional)\n",
    "              (dog_ears_w, dog_ears_h, dog_ears_channels) = dog_ears_image.shape[:3]\n",
    "              (dog_nose_w, dog_nose_h, dog_nose_channels) = dog_nose_image.shape[:3]\n",
    "              scale_factor = (left_ear_x - right_ear_x) / dog_ears_w  # Adjust scaling as needed\n",
    "              dog_ears_resized = cv2.resize(dog_ears_image, None, fx=scale_factor, fy=scale_factor)\n",
    "              dog_nose_resized = cv2.resize(dog_nose_image, None, fx=scale_factor, fy=scale_factor)\n",
    "\n",
    "              # Get ROI (Region of Interest) for placing the images\n",
    "              dog_ears_roi = image[left_ear_y:left_ear_y + dog_ears_resized.shape[0], left_ear_x:left_ear_x + dog_ears_resized.shape[1]]\n",
    "              dog_nose_roi = image[nose_tip_y:nose_tip_y + dog_nose_resized.shape[0], nose_tip_x:nose_tip_x + dog_nose_resized.shape[1]]\n",
    "\n",
    "              # Create masks for dog ears and nose (assuming PNG with alpha channel)\n",
    "              dog_ears_mask = dog_ears_resized[:, :, 3] if dog_ears_channels == 4 else dog_ears_resized[:, :]\n",
    "              dog_nose_mask = dog_nose_resized[:, :, 3] if dog_nose_channels == 4 else dog_nose_resized[:, :]\n",
    "\n",
    "              # Apply bitwise AND with masks to remove unwanted background\n",
    "              dog_ears_roi = cv2.bitwise_and(dog_ears)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "402c2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe face mesh and drawing utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Load dog ears and nose images (modify paths as needed)\n",
    "dog_ears_image = cv2.imread(\"C:/image/png-transparent-dog-ears-nose-and-tongue-illustration-dalmatian-dog-snapchat-graphic-filter-snapchat-filters-s-pet-application-software-transparency-and-translucency-thumbnail.png\", cv2.IMREAD_UNCHANGED)\n",
    "dog_nose_image = cv2.imread(\"C:/image/png-transparent-dog-ears-nose-and-tongue-illustration-dalmatian-dog-snapchat-graphic-filter-snapchat-filters-s-pet-application-software-transparency-and-translucency-thumbnail.png\", cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)  # Change 0 to video file path if needed\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    success, image = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        # If loading video, check for end of stream\n",
    "        break\n",
    "\n",
    "    # Convert the BGR image to RGB format\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Run MediaPipe face mesh\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        max_num_faces=1, min_detection_confidence=0.5, min_tracking_confidence=0.5\n",
    "    ) as face_mesh:\n",
    "        results = face_mesh.process(image_rgb)\n",
    "\n",
    "        # Draw face landmarks and contours (if faces detected)\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                # Access specific landmark coordinates (modify for precise placement)\n",
    "                left_ear_x = int(face_landmarks.landmark[234].x * image.shape[1])  # Index 234 corresponds to the left ear\n",
    "                left_ear_y = int(face_landmarks.landmark[234].y * image.shape[0])\n",
    "                right_ear_x = int(face_landmarks.landmark[454].x * image.shape[1])  # Index 454 corresponds to the right ear\n",
    "                right_ear_y = int(face_landmarks.landmark[454].y * image.shape[0])\n",
    "                nose_tip_x = int(face_landmarks.landmark[2].x * image.shape[1])     # Index 2 corresponds to the nose tip\n",
    "                nose_tip_y = int(face_landmarks.landmark[2].y * image.shape[0])\n",
    "\n",
    "                # Resize dog ears and nose images based on face size (optional)\n",
    "                scale_factor_ears = abs(left_ear_x - right_ear_x) / dog_ears_image.shape[1]  # Adjust scaling as needed\n",
    "                scale_factor_nose = abs(left_ear_x - right_ear_x) / dog_nose_image.shape[1]  # Adjust scaling as needed\n",
    "                dog_ears_resized = cv2.resize(dog_ears_image, None, fx=scale_factor_ears, fy=scale_factor_ears)\n",
    "                dog_nose_resized = cv2.resize(dog_nose_image, None, fx=scale_factor_nose, fy=scale_factor_nose)\n",
    "\n",
    "                # Get ROI (Region of Interest) for placing the images\n",
    "                dog_ears_roi = image[left_ear_y:left_ear_y + dog_ears_resized.shape[0], left_ear_x:left_ear_x + dog_ears_resized.shape[1]]\n",
    "                dog_nose_roi = image[nose_tip_y:nose_tip_y + dog_nose_resized.shape[0], nose_tip_x:nose_tip_x + dog_nose_resized.shape[1]]\n",
    "\n",
    "                # Create masks for dog ears and nose (assuming PNG with alpha channel)\n",
    "                dog_ears_mask = dog_ears_resized[:, :, 3] if dog_ears_image.shape[2] == 4 else None\n",
    "                dog_nose_mask = dog_nose_resized[:, :, 3] if dog_nose_image.shape[2] == 4 else None\n",
    "\n",
    "                # Apply bitwise AND with masks to remove unwanted background\n",
    "                if dog_ears_mask is not None:\n",
    "                    dog_ears_roi = cv2.bitwise_and(dog_ears_resized[:, :, :3], dog_ears_mask)\n",
    "                if dog_nose_mask is not None:\n",
    "                    dog_nose_roi = cv2.bitwise_and(dog_nose_resized[:, :, :3], dog_nose_mask)\n",
    "\n",
    "                # Combine dog ears and nose with the frame\n",
    "                image[left_ear_y:left_ear_y + dog_ears_roi.shape[0], left_ear_x:left_ear_x + dog_ears_roi.shape[1]] = dog_ears_roi\n",
    "                image[nose_tip_y:nose_tip_y + dog_nose_roi.shape[0], nose_tip_x:nose_tip_x + dog_nose_roi.shape[1]] = dog_nose_roi\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Dog Filter\", image)\n",
    "\n",
    "    # Quit if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238439e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
